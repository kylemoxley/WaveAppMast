{"cells":[{"cell_type":"code","source":["%run ./WaveSetEnvironment1"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import pyspark.sql.functions as f"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# SLA Data\nsla_data = spark.read.format('csv') \\\n  .option(\"header\", 'true') \\\n  .option(\"sep\", ',') \\\n  .option(\"inferSchema\", \"true\") \\\n  .option(\"dateFormat\",\"yyyy/MM/dd HH\")\\\n  .load(SLA_PATH) \\\n  .withColumnRenamed(\"PickTicketCreateDateTime\",\"Date\") \\\n  .withColumnRenamed(\"At Once\",\"AtOnce\") \\\n  .drop(\"PickTicketControlNumber\").drop(\"StartShipdateKey\")\n\n# #Aggregations of SLA Data\nsla_staging_sum = sla_data.groupBy(\"WaveNumber\") \\\n  .sum(\"SLAMet\")\ndisplay(sla_data)\n# # sla_staging_agg = sla_data.groupBy(\"WaveNumber\") \\\n\n#   .avg(\"Month\")\\\n#   .avg(\"Year\")\\\n#   .avg(\"Weekday\")\\\n#   .avg(\"Week\")\\\n#   .avg(\"WKDay\")\\\n#   .avg(\"Hour\")\\\n#   .avg(\"Current\")\\\n#   .avg(\"Future\")\\\n#   .avg(\"AtOnce\")\n  \n\n# sla_staging_count = sla_data.groupBy(\"WaveNumber\") \n#   .agg(lit(func.count(\"SLAMet\")).alias(\"count_alias\")) \n#   .agg(func.(\"Month\"))\n#   .agg(lit(func.avg(\"Year\")))\\\n#   .agg(lit(func.avg(\"Weekday\")))\\\n#   .agg(lit(func.avg(\"Week\")))\\\n#   .agg(lit(func.avg(\"WKDay\")))\\\n#   .agg(lit(func.avg(\"Hour\")))\\\n#   .agg(lit(func.avg(\"Current\")))\\\n#   .agg(lit(func.avg(\"Future\")))\\\n#   .agg(lit(func.avg(\"AtOnce\")))\n  \n# sla_staging = sla_staging_sum.join(sla_staging_count,\"WaveNumber\") \\\n#   .withColumn(\"Ratio\",col(\"sum(SLAMet)\")/col(\"count_alias\")) \\\n#   .withColumn(\"y\", when(col(\"Ratio\") < 1.0, 0).otherwise(1)) \\\n#   .drop(\"sum(SLAMet)\").drop(\"count_alias\")\n\n\n# #Joining SLAback\n# sla_data = sla_staging.join(sla_data, \"WaveNumber\")\n# display(sla_data)\n\n#wave data\nwave_data = spark.read.format('csv') \\\n  .option(\"header\", 'true') \\\n  .option(\"sep\", ',') \\\n  .option(\"inferSchema\", \"true\") \\\n  .option(\"dateFormat\",\"yyyy-MM-dd HH\") \\\n  .load(WAVE_PATH) \\\n  .drop(\"WaveType\").drop(\"WaveTypeDesc\").drop(\"WaveStatus\").drop(\"ModificationDateTime\").drop(\"_c0\")\n# Join SLA to Wave\nwave_to_lake =wave_data.join(sla_data, \"WaveNumber\")\ndisplay(wave_data)\n\n              \n              \n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Throughput\n \nThroughPut = spark.read.format('csv') \\\n  .option(\"header\", 'true') \\\n  .option(\"sep\", ',') \\\n  .option(\"inferSchema\", \"true\") \\\n  .option(\"dateFormat\",\"yyyy/MM/dd HH:mm\")\\\n  .load(ThroughPut_Path) \\\n  .withColumnRenamed(\"EndDateTime\",\"Date\") \\\n  .withColumnRenamed(\"APlusStocking\",\"TPAPlusStocking\") \\\n  .withColumnRenamed(\"APlusPicking\",\"TPAPlusPicking\") \\\n  .withColumnRenamed(\"BatchWall\",\"TPBatchWall\") \\\n  .withColumnRenamed(\"HighBay\",\"TPHighBay\") \\\n  .withColumnRenamed(\"SpecialHandling\",\"TPSpecialHandling\")\\\n  .withColumn(\"TotalThroughPut\",(col(\"TPAPlusPicking\")+col(\"TPSpecialHandling\")/2)/100)\\\n  .drop(\"_c0\")\ndisplay(ThroughPut)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Join SLA/WaveData to Throughput\ndata_to_lake = ThroughPut.join(wave_to_lake, \"Date\")\n\ndisplay(data_to_lake)\n# #Write Main Data Set\ndata_to_lake.write.format('delta') \\\n  .mode(\"overwrite\") \\\n  .option('overwriteSchema', 'true') \\\n  .save(DIR + 'ThroughPut_Train_Dtt.delta')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["data_to_lake"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"WaveETLMaster","notebookId":1771860058921289},"nbformat":4,"nbformat_minor":0}
